Completed tasks:
Sqoop  Import into DataLake - Reusable Script

Date 9/July/2018
Reconciliation step to check Mysql and Hive count
------------------------------
104.196.40.14:3306
roo 
root
hive Service -> hive --service hiveServer2
Connect via 104.196.40.14:10000 using beeline(JDBC client). or Remote login into edge node and access hive using "hive"I thrift client)
-------------------------------
Today: Process Data using RDD,DF(DSL) , Spark SQL
i) Check hive table data delimiters and File Format

which git
git clone https://github.com/datamitra/notes.git
mv  project notes/
git add *
git commit -m "first commit"
git push
git rm -r to remove  a folder
https://help.rallydev.com - provide requirement - user stories 
Agile methodology -> Sprint ->Scrum calls, Scrum master
---------------------------------------
Questions; 
Each customer How many orders present over time.
query 1) customer - Over time -count (orders) - value of orders



Query 2) From Each Office location Number of orders over time with order status.

Question:: office, year, month , orders_quantity , group by status  and Sales made
How much amount of sales each office made?
--------------------
Query 2 solution:::

This query involves in orderdetails,orders,customers,employees,offices.


i) number of orders quantity status wise 
select orderNumber,quantityOrdered,priceEach from orderdetails;
calculate the amoutn for every order number.
select orderNumber,quantityOrdered,priceEach, 
quantityOrdered*priceEach as totalAmount  from orderdetails;

Observe that orderNumber and productid is the primary key. 
So for a give order, group all the results.
--------------------------------------------------
Question:: office, year, month , orders_quantity , group by status  and total sales it made.

table od
Query::  quantityOrdered, sales for each orderNumber
select orderNumber,  sum(quantityOrdered),sum(quantityOrdered*priceEach) as totalAmount   from orderdetails where orderNumber=10100 group by orderNumber;


---------------------------------------------------
From orders table:::
 orderNumber, orderDate (year,month), status , customerNumber
select orderNumber,orderDate,year(orderDate) yr,month(orderDate) month,customerNumber from orders;

RDD, DF, DS, SQL -> 

Each office and Sales amount:::
--------------------
table orders
select orderNumber,customerNumber
from orders;
--------------------


select orderNumber,  sum(quantityOrdered),sum(quantityOrdered*priceEach) as totalAmount   
from orderdetails 
where orderNumber=10100 group by orderNumber;




select 
count(O.orderNumber),
C.customerNumber
from orders O join customers  C on (C.customerNumber=O.customerNumber)
group by O.customerNumber





Date: 11 July 2018
Task  is solve this using RDDs and DF DSL, use DS and SQL
--------------------------------------------------------
 sh startdaemons.sh
 mysql -uroot -proot -e "show tables in classicmodels"
customers,employees,offices,orderdetails,orders,payments,productlines, products
sh sqoop_import_hive_table.sh products

hive>select current_database();
hive_count=`hive -S -e "select count(1) from hdpdlake.products"`
mysql_count=`mysql -uroot -proot -e "select count(1) from classicmodels.products"`
echo $hive_count  --- $mysql_count
118 --- coun110t(1) 
WHY ???

Now orderdetails:::::
sh sqoop_import_hive_table.sh orderdetails
hive_count=`hive -S -e "select count(1) from hdpdlake.orderdetails"` 
mysql_count=`mysql -uroot -proot -e "select count(1) from classicmodels.orderdetails" `
echo $hive_count  --- $mysql_count
2996 --- count(1) 2996
hive -e "desc formatted hdpdlake.orderdetails" | grep -i location

hdfs://localhost:9000/hdpdlake/orderdetails
hdfs dfs -cat  hdfs://localhost:9000/hdpdlake/orderdetails/*

hive>desc formatted products;
Delmiter is controlA. 

Create a table csv format and load data into csv ..
show create table  hdpdlake.orderdetails;
OK
CREATE TABLE `hdpdlake.orderdetails`(
  `ordernumber` int,
  `productcode` string,
  `quantityordered` int,
  `priceeach` double,
  `orderlinenumber` int)
COMMENT 'Imported by sqoop on 2018/07/04 15:58:30'
ROW FORMAT SERDE
  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
WITH SERDEPROPERTIES (
  'field.delim'='',
  'line.delim'='\n',
  'serialization.format'='')
STORED AS INPUTFORMAT
  'org.apache.hadoop.mapred.TextInputFormat'
OUTPUTFORMAT
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
  'hdfs://localhost:9000/hdpdlake/orderdetails'
TBLPROPERTIES (
  'transient_lastDdlTime'='1531274874')
 
 CREATE TABLE `hdpdlake.orderdetails_csv`(
  `ordernumber` int,
  `productcode` string,
  `quantityordered` int,
  `priceeach` double,
  `orderlinenumber` int)
row format delimited fields terminated by ","
stored as textfile;

I want to have every field enclosed by double quotes, so that it can be consumed by external tools like informatica.

CREATE TABLE hdpdlake.orderdetails_csv(a string, b string, ...)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
   "separatorChar" = "\t",
   "quoteChar"     = "'",
   "escapeChar"    = "\\"
)  
STORED AS TEXTFILE;
DEFAULT_ESCAPE_CHARACTER \
DEFAULT_QUOTE_CHARACTER  "
DEFAULT_SEPARATOR        ,

--------------------------------------
SELECT *
FROM orders
INTO OUTFILE '/home/dorababugjntup/notes/project/data/orders.csv'
FIELDS TERMINATED BY ','
ENCLOSED BY '"'
LINES TERMINATED BY '\n'

mysqldump -uroot -proot --no-create-db  --tab /home/dorababugjntup/notes/project/data/  --fields-enclosed-by='"'  --fields-terminated-by=','  --databases classicmodels --tables orders 



ERROR 1290 (HY000): The MySQL server is running with the --secure-file-priv option so it cannot execute this statement

SHOW VARIABLES LIKE "secure_file_priv";

sqoop import  \
--connect jdbc:mysql://localhost:3306/$mysql_dbname?zeroDateTimeBehavior=CONVERT_TO_NULL \
--username $mysql_user \
--password $mysql_password \
--table $1 \
--delete-target-dir \
--target-dir "/home/dorababugjntup/notes/project/data/$1.csv" \
--as-textfile \
-m 1 \
--enclosed-by '"' \
--fields-terminated-by "," \



#read option
echo "*** Table name is :: $1 "

sqoop import  \
--connect jdbc:mysql://localhost:3306/classicmodels?zeroDateTimeBehavior=CONVERT_TO_NULL \
--username root \
--password root \
--table $1 \
--delete-target-dir \
--target-dir "/classicmodels/$1" \
--as-textfile \
-m 1 \
--enclosed-by '"' \
--fields-terminated-by "," \
--mysql-delimiters \

customers,employees, offices  , orderdetails        ,orders   , payments ,productlines,products

sh sqoop_dump_table.sh customers &
nohup sh sqoop_dump_table.sh employees &
nohup sh sqoop_dump_table.sh offices &
nohup sh sqoop_dump_table.sh orderdetails &
nohup sh sqoop_dump_table.sh orders &
nohup sh sqoop_dump_table.sh payments &
nohup sh sqoop_dump_table.sh productlines &
nohup sh sqoop_dump_table.sh products &

 select REPLACE(REPLACE(productdescription, '\r', ''), '\n', '')  from products;


hdfs dfs -cat /classicmodels/customers/*
hdfs dfs -cat /classicmodels/employees/*
hdfs dfs -cat /classicmodels/offices/*
hdfs dfs -cat /classicmodels/orderdetails/*
hdfs dfs -cat /classicmodels/orders/*
hdfs dfs -cat /classicmodels/payments/*
hdfs dfs -cat /classicmodels/productlines/*
hdfs dfs -cat /classicmodels/products/*
-----------------------------------------------
13 July 2018::::::::
Data is in HDFS - in the form of CSV Files - All files are enclosed by double quotes.
So Start reading the files by CSV serde.


CREATE TABLE `products_csv`(
  `productcode` string,
  `productname` string,
  `productline` string,
  `productscale` string,
  `productvendor` string,
  `productdescription` string,
  `quantityinstock` int,
  `buyprice` double,
  `msrp` double)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde';

hdfs dfs -cp /classicmodels/products/* /hdpdlake/products_csv/
-----------------------------------------
All jobs are running with dr.who..
https://community.hortonworks.com/questions/189402/why-are-there-drwho-myyarn-applications-running-an.html

crontab -e, commented all the lines. Restarted resouce manager.
still coming


https://community.hortonworks.com/questions/2349/tip-when-you-get-a-message-in-job-log-user-dr-who.html
------------------------------------------

 ps -ef | grep sqoop_dump_table.sh

 kill -9 20401 , the second column.
 -----------------------------------------



